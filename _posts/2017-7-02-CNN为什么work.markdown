---
layout:     post
title:      "CNN为什么有效"
date:       2017-7-2 21:30:00
author:     "liangyu"
header-img: "img/pmUEwPKL5IE.jpg"
tags:
    - Deep Learning
---

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->
<!-- code_chunk_output -->

* [前言](#前言)
* [正文](#正文)
	* [宏观解释](#宏观解释)
	* [局部相关性](#局部相关性)
	* [空间不变性](#空间不变性)
	* [CNN具体实现](#cnn具体实现)
	* [池化方式](#池化方式)
	* [神经科学的解释](#神经科学的解释)
* [后记](#后记)
* [参考文献](#参考文献)

<!-- /code_chunk_output -->

## 前言

最近的项目在用TensorFlow搭建卷积神经网络，网上很多教程都只说了常用的CNN结构，比如AlexNet, VGG16等，但对于**CNN为什么work，以及适用于什么样的数据**并没有说明，反正做图像大家都用，我也用就是了，但是这样总觉得不透彻，知其然而不知其所以然。

我在网上查了一些资料，主要是知乎上，也看了一些论文，现在结合自己的理解说一说。

## 正文

### 宏观解释

首先从宏观的角度理解CNN为什么work？

1. 从统计角度：卷积抓住了问题的**局部相关性**和**空间不变性**
2. 从正则化的角度：由于权值共享，降低了模型参数数量，控制了模型复杂度
3. 从神经科学角度：卷积神经网络受生物视觉系统的启发

### 局部相关性

CNN不仅应用于图像领域，同时应用于语音和自然语言处理领域，这些数据的共同特点是具有**局部相关性**。什么是局部相关性？具体来说，图像是由一个个像素点组成，每个像素点与其周围的像素点是有关联的，如果把像素点打乱，图片会完全变掉，语音和自然语言也是同理，我们不能随意打乱这些数据，因为它们都具有局部相关性。

### 空间不变性

CNN浅层网络提取低层次的特征，比如边缘，曲线等，随着网络深度加深，低层次的特征经过组合组成高层次的特征，并且能够保持特征的**空间不变性**。

### CNN具体实现

CNN通过四个手段抓住局部相关性和空间不变性：

* 局部连接
* 权值共享
* 池化操作
* 多层次结构

局部连接使CNN能够提取数据的局部特征；权值共享大大减小了网络参数，每个卷积核是一个特征提取器，只提取一种特征，卷积的过程就是在图片上每个局部区域做模板匹配；池化操作是一种下采样，特征的组合是判断原始图像是什么的关键，特征的绝对位置不是很重要，不同特征之间的**相对位置**更重要，使用相对位置可以对一些平移，旋转更鲁棒，控制过拟合，增加模型的泛化能力，同时大大减小了模型参数；多层次结构将局部低层次特征组合为高层次特征。

### 池化方式

CNN中常用的池化方式：

* 最大池化（Max pooling）：在图像分类和目标检测中最常用
* 平均池化（Average pooling）

### 神经科学的解释

1959年，Hubel和Wiesel的实验表明生物的视觉处理是从简单的形状开始的，比如边缘，直线，曲线，并凭借这一发现获得了诺贝尔奖，这也为CNN提供一定的理论支持。其实深度学习领域的很多trick都可以找到这种生物神经学的理论支持。

## 后记

主要梳理了CNN为什么work，通过什么work，相关的理论支持，以及CNN适合处理什么样的数据。

## 参考文献

1. [如果你是面试官，你怎么去判断一个面试者的深度学习水平？](https://www.zhihu.com/question/41233373/answer/91113816)
2. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)

***

本作品采用 <a rel="license" href="http://creativecommons.org/licenses/by/3.0/cn/">知识共享署名 3.0 中国大陆许可协议</a> 进行许可。
